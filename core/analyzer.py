"""
í”¼ë…¸í‚¤ì˜¤ í”„ë¡œì íŠ¸ â€” ë©€í‹°ëª¨ë‹¬ ë¶„ì„ ì—”ì§„ v3.3 (Phase 3)
ì˜ìƒ 6ê°œ + ìŒì„± 7ê°œ = 13ê°œ ì§€í‘œ â†’ ë³µí•© ìŠ¤ì½”ì–´ + 7ê°ì • ë¶„ë¥˜

ê°ì • ë¶„ë¥˜: Blendshapes ê¸°ë°˜ ê·œì¹™ ì—”ì§„
ë³µí•© ìŠ¤ì½”ì–´: ë² ì´ìŠ¤ë¼ì¸ ëŒ€ë¹„ ë³€í™”ìœ¨ ê°€ì¤‘ í•©ì‚°
"""
import numpy as np
from dataclasses import dataclass, field
from typing import Optional
from config.settings import SCORE_WEIGHTS, THRESHOLDS


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 7ê°ì • ì •ì˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
EMOTIONS = ["happy", "sad", "angry", "surprise", "fear", "disgust", "neutral"]

EMOTION_LABELS_KR = {
    "happy": "í–‰ë³µ ğŸ˜Š",
    "sad": "ìŠ¬í”” ğŸ˜¢",
    "angry": "ë¶„ë…¸ ğŸ˜ ",
    "surprise": "ë†€ëŒ ğŸ˜²",
    "fear": "ë¶ˆì•ˆ ğŸ˜°",
    "disgust": "ë¶ˆì¾Œ ğŸ˜–",
    "neutral": "ë³´í†µ ğŸ˜",
}


@dataclass
class AnalysisResult:
    """ë©€í‹°ëª¨ë‹¬ ì¢…í•© ë¶„ì„ ê²°ê³¼"""
    # ê°œë³„ ìŠ¤ì½”ì–´ (0~100)
    vision_score: float = 0.0
    voice_score: float = 0.0
    combined_score: float = 0.0

    # ê°ì • ë¶„ë¥˜
    emotion: str = "neutral"
    emotion_kr: str = "ë³´í†µ ğŸ˜"
    emotion_confidence: float = 0.0
    emotion_scores: dict = field(default_factory=dict)

    # ìŠ¤íŠ¸ë ˆìŠ¤ ë ˆë²¨
    stress_level: str = "low"
    anomaly_flags: list = field(default_factory=list)

    # ì˜ìƒ ì§€í‘œ ì„œë¸Œ ìŠ¤ì½”ì–´ (ë””ë²„ê·¸ìš©)
    sub_vision: dict = field(default_factory=dict)
    sub_voice: dict = field(default_factory=dict)


class Analyzer:
    """
    ë©€í‹°ëª¨ë‹¬ í“¨ì „ ë¶„ì„ê¸° v3.3
    ì˜ìƒ 6ê°œ + ìŒì„± 7ê°œ = 13ê°œ ì§€í‘œ â†’ ë³µí•© ìŠ¤ì½”ì–´ + 7ê°ì • ë¶„ë¥˜
    """

    def __init__(self):
        self.baseline_vision = {}
        self.baseline_voice = {}
        self._baseline_ready = False

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ë©”ì¸ ë¶„ì„
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def analyze(self, face_data, voice_data=None, blink_rate: float = 0.0,
                iris_instability: float = 0.0) -> AnalysisResult:
        """
        ì¢…í•© ë¶„ì„

        Args:
            face_data: FaceData (vision.py)
            voice_data: VoiceData (voice_analyzer.py)
            blink_rate: ê¹œë¹¡ì„/ë¶„ (VisionEngine.get_blink_rate())
            iris_instability: í™ì±„ ë¶ˆì•ˆì •ë„ (VisionEngine.get_iris_instability())

        Returns:
            AnalysisResult
        """
        result = AnalysisResult()

        if not face_data.detected:
            return result

        # â”€â”€ 1. ê°ì • ë¶„ë¥˜ (Blendshapes ê¸°ë°˜) â”€â”€
        emotion_scores = self._classify_emotion(face_data)
        result.emotion_scores = emotion_scores

        top_emotion = max(emotion_scores, key=emotion_scores.get)
        result.emotion = top_emotion
        result.emotion_kr = EMOTION_LABELS_KR.get(top_emotion, "ë³´í†µ ğŸ˜")
        result.emotion_confidence = emotion_scores[top_emotion]

        # â”€â”€ 2. ì˜ìƒ ìŠ¤ì½”ì–´ (6ê°œ ì§€í‘œ) â”€â”€
        vision_sub = self._compute_vision_score(face_data, blink_rate, iris_instability)
        result.sub_vision = vision_sub
        result.vision_score = self._weighted_sum(vision_sub, SCORE_WEIGHTS["vision"])

        # â”€â”€ 3. ìŒì„± ìŠ¤ì½”ì–´ (7ê°œ ì§€í‘œ) â”€â”€
        if voice_data and voice_data.f0_mean > 0:
            voice_sub = self._compute_voice_score(voice_data)
            result.sub_voice = voice_sub
            result.voice_score = self._weighted_sum(voice_sub, SCORE_WEIGHTS["voice"])

        # â”€â”€ 4. ë©€í‹°ëª¨ë‹¬ í“¨ì „ â”€â”€
        fusion = SCORE_WEIGHTS["fusion"]
        if voice_data and voice_data.f0_mean > 0:
            result.combined_score = (
                result.vision_score * fusion["vision"]
                + result.voice_score * fusion["voice"]
            )
        else:
            # ìŒì„± ì—†ìœ¼ë©´ ì˜ìƒë§Œ ì‚¬ìš©
            result.combined_score = result.vision_score

        result.combined_score = min(100, max(0, result.combined_score))

        # â”€â”€ 5. ìŠ¤íŠ¸ë ˆìŠ¤ ë ˆë²¨ â”€â”€
        if result.combined_score < THRESHOLDS["stress_low"]:
            result.stress_level = "low"
        elif result.combined_score < THRESHOLDS["stress_high"]:
            result.stress_level = "medium"
        else:
            result.stress_level = "high"

        # â”€â”€ 6. ì´ìƒ ê°ì§€ í”Œë˜ê·¸ â”€â”€
        result.anomaly_flags = self._detect_anomalies(face_data, voice_data, blink_rate)

        return result

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ê°ì • ë¶„ë¥˜ (Blendshapes ê·œì¹™ ì—”ì§„)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def _classify_emotion(self, face_data) -> dict:
        """
        Blendshapes ê¸°ë°˜ 7ê°ì • ë¶„ë¥˜
        ê° ê°ì •ë³„ ì ìˆ˜(0~1)ë¥¼ ë°˜í™˜, softmax ì •ê·œí™”
        """
        scores = {e: 0.0 for e in EMOTIONS}
        bs = face_data.blendshapes
        if not bs:
            scores["neutral"] = 1.0
            return scores

        # â”€â”€ Happy â”€â”€
        smile_l = bs.get("mouthSmileLeft", 0)
        smile_r = bs.get("mouthSmileRight", 0)
        cheek_l = bs.get("cheekSquintLeft", 0)
        cheek_r = bs.get("cheekSquintRight", 0)
        scores["happy"] = (smile_l + smile_r) / 2 * 0.6 + (cheek_l + cheek_r) / 2 * 0.4

        # â”€â”€ Sad â”€â”€
        frown_l = bs.get("mouthFrownLeft", 0)
        frown_r = bs.get("mouthFrownRight", 0)
        brow_inner = bs.get("browInnerUp", 0)
        scores["sad"] = (frown_l + frown_r) / 2 * 0.5 + brow_inner * 0.5

        # â”€â”€ Angry â”€â”€
        brow_down_l = bs.get("browDownLeft", 0)
        brow_down_r = bs.get("browDownRight", 0)
        mouth_press_l = bs.get("mouthPressLeft", 0)
        mouth_press_r = bs.get("mouthPressRight", 0)
        jaw_clench = bs.get("jawForward", 0)
        scores["angry"] = (
            (brow_down_l + brow_down_r) / 2 * 0.4
            + (mouth_press_l + mouth_press_r) / 2 * 0.3
            + jaw_clench * 0.3
        )

        # â”€â”€ Surprise â”€â”€
        eye_wide_l = bs.get("eyeWideLeft", 0)
        eye_wide_r = bs.get("eyeWideRight", 0)
        brow_outer_l = bs.get("browOuterUpLeft", 0)
        brow_outer_r = bs.get("browOuterUpRight", 0)
        jaw_open = bs.get("jawOpen", 0)
        scores["surprise"] = (
            (eye_wide_l + eye_wide_r) / 2 * 0.3
            + brow_inner * 0.2
            + (brow_outer_l + brow_outer_r) / 2 * 0.2
            + jaw_open * 0.3
        )

        # â”€â”€ Fear â”€â”€
        scores["fear"] = (
            brow_inner * 0.3
            + (eye_wide_l + eye_wide_r) / 2 * 0.3
            + (mouth_press_l + mouth_press_r) / 2 * 0.2
            + face_data.lip_press_score * 0.2
        )

        # â”€â”€ Disgust â”€â”€
        nose_l = bs.get("noseSneerLeft", 0)
        nose_r = bs.get("noseSneerRight", 0)
        upper_lip = bs.get("mouthShrugUpper", 0)
        scores["disgust"] = (
            (nose_l + nose_r) / 2 * 0.5
            + upper_lip * 0.3
            + (frown_l + frown_r) / 2 * 0.2
        )

        # â”€â”€ Neutral â”€â”€
        # ë‹¤ë¥¸ ê°ì •ì´ ëª¨ë‘ ë‚®ìœ¼ë©´ neutral
        max_others = max(scores[e] for e in EMOTIONS if e != "neutral")
        scores["neutral"] = max(0, 0.5 - max_others)

        # softmax ì •ê·œí™”
        total = sum(scores.values())
        if total > 0:
            scores = {k: round(v / total, 3) for k, v in scores.items()}
        else:
            scores["neutral"] = 1.0

        return scores

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ì˜ìƒ ìŠ¤ì½”ì–´ (6ê°œ ì§€í‘œ â†’ 0~100)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def _compute_vision_score(self, face_data, blink_rate, iris_instability) -> dict:
        """ê° ì˜ìƒ ì§€í‘œë¥¼ 0~100 ìŠ¤ì½”ì–´ë¡œ ë³€í™˜"""
        sub = {}

        # 1. í™ì±„ ë¶ˆì•ˆì •ë„ (ê¸°ì¡´ stress_score)
        sub["iris_instability"] = min(100, iris_instability)

        # 2. ê¹œë¹¡ì„ ë³€í™”ìœ¨
        lo, hi = THRESHOLDS["blink_normal_range"]
        mid = (lo + hi) / 2
        if blink_rate > 0:
            blink_dev = abs(blink_rate - mid) / mid * 100
            sub["blink_rate_change"] = min(100, blink_dev)
        else:
            sub["blink_rate_change"] = 0

        # 3. ë™ê³µ í™•ì¥ë¥ 
        sub["pupil_dilation"] = min(100, abs(face_data.pupil_dilation_pct) * 5)

        # 4. ë¯¸ì„¸ í‘œì •
        mc = face_data.micro_expression_count
        sub["micro_expression"] = min(100, mc * 20)  # 5ê±´ì´ë©´ 100

        # 5. ë¹„ëŒ€ì¹­
        asym = face_data.asymmetry_score
        sub["asymmetry"] = min(100, asym / THRESHOLDS["asymmetry_threshold"] * 50)

        # 6. ì…ìˆ  ì••ì¶•
        lip = face_data.lip_press_score
        sub["lip_press"] = min(100, lip / THRESHOLDS["lip_press_threshold"] * 50)

        return sub

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ìŒì„± ìŠ¤ì½”ì–´ (7ê°œ ì§€í‘œ â†’ 0~100)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def _compute_voice_score(self, voice_data) -> dict:
        """ê° ìŒì„± ì§€í‘œë¥¼ 0~100 ìŠ¤ì½”ì–´ë¡œ ë³€í™˜"""
        sub = {}

        # 1. í”¼ì¹˜ ë³€í™” (F0 std ê¸°ë°˜)
        sub["pitch_change"] = min(100, voice_data.f0_std * 2)

        # 2. ì‘ë‹µ ì§€ì—°
        lat = voice_data.response_latency_sec
        if lat > THRESHOLDS.get("response_latency_stress", 3.0):
            sub["response_latency"] = min(100, lat * 20)
        elif lat > THRESHOLDS.get("response_latency_normal", 2.0):
            sub["response_latency"] = min(100, lat * 15)
        else:
            sub["response_latency"] = max(0, lat * 5)

        # 3. ë¹„ìœ ì°½ì„±
        sub["disfluency"] = min(100, voice_data.disfluency_total * 25)

        # 4. Jitter
        sub["jitter_change"] = min(100, voice_data.jitter_pct * 10)

        # 5. ë§ ì†ë„ ë³€í™”
        if voice_data.speech_rate > 0:
            # ì •ìƒ í•œêµ­ì–´ ë§ ì†ë„: 3~5 ìŒì ˆ/ì´ˆ
            rate_dev = abs(voice_data.speech_rate - 4.0) / 4.0 * 100
            sub["speech_rate_change"] = min(100, rate_dev)
        else:
            sub["speech_rate_change"] = 0

        # 6. ìŒëŸ‰ ë³€í™”
        sub["volume_change"] = min(100, voice_data.volume_std * 500)

        return sub

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ì´ìƒ ê°ì§€
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def _detect_anomalies(self, face_data, voice_data, blink_rate) -> list:
        """ì´ìƒ íŒ¨í„´ ê°ì§€ â†’ í”Œë˜ê·¸ ë¦¬ìŠ¤íŠ¸"""
        flags = []

        # ì‹œì„  íšŒí”¼ ê³¼ë‹¤
        if face_data.gaze_aversion_count > 8:
            flags.append("ì‹œì„  íšŒí”¼ ë¹ˆë²ˆ")

        # ê¹œë¹¡ì„ ê¸‰ì¦
        lo, hi = THRESHOLDS["blink_normal_range"]
        if blink_rate > hi * 1.5:
            flags.append("ê¹œë¹¡ì„ ê¸‰ì¦")

        # ë¯¸ì„¸ í‘œì • ë‹¤ë°œ
        if face_data.micro_expression_count >= 5:
            flags.append("ë¯¸ì„¸ í‘œì • ë‹¤ë°œ")

        # ì…ìˆ  ì••ì¶• ì§€ì†
        if face_data.lip_press_score > THRESHOLDS["lip_press_threshold"] * 1.5:
            flags.append("ì…ìˆ  ì••ì¶• ê°•í•¨")

        # ìŒì„± ì´ìƒ
        if voice_data and voice_data.f0_mean > 0:
            if voice_data.response_latency_sec > 5.0:
                flags.append("ì‘ë‹µ ì§€ì—° ê³¼ë‹¤")
            if voice_data.disfluency_total >= 4:
                flags.append("ë¹„ìœ ì°½ì„± ê³¼ë‹¤")

        return flags

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ìœ í‹¸ë¦¬í‹°
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    @staticmethod
    def _weighted_sum(sub_scores: dict, weights: dict) -> float:
        """ê°€ì¤‘ í•©ì‚°"""
        total = 0.0
        for key, weight in weights.items():
            total += sub_scores.get(key, 0) * weight
        return total

    def set_baseline(self, vision_baseline: dict, voice_baseline: dict = None):
        """ë² ì´ìŠ¤ë¼ì¸ ì„¤ì •"""
        self.baseline_vision = vision_baseline
        if voice_baseline:
            self.baseline_voice = voice_baseline
        self._baseline_ready = True
        print(f"[Analyzer] ë² ì´ìŠ¤ë¼ì¸ ì„¤ì • ì™„ë£Œ (ì˜ìƒ: {len(vision_baseline)}í•­ëª©)")

    @property
    def is_baseline_ready(self) -> bool:
        return self._baseline_ready
